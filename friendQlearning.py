# -*- coding: utf-8 -*-
"""FriendQ.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1UsLgfMh5ylSuYZehsAxUMyX5ugz9PhGA
"""

import random
import gym
from gym import error, spaces, utils
import numpy as np
import os
import matplotlib.pyplot as plt
#from gym.spaces import Box
from gym.spaces import Discrete
from math import exp
import sys
import seaborn as sns
import pylab
import pandas as pd
import cv2
import imageio
from datetime import datetime



# basic moves every agent should do
AGENT_ACTIONS = {0: 'MOVE_LEFT',  # Move left
                1: 'MOVE_RIGHT',  # Move right
                2: 'MOVE_UP',  # Move up
                3: 'MOVE_DOWN',  # Move down
                4: 'STAY'  # don't move
                }  # Rotate clockwise

ACTIONS = {'MOVE_LEFT': [0, -1],  # Move left
           'MOVE_RIGHT': [0, 1],  # Move right
           'MOVE_UP': [-1, 0],  # Move up
           'MOVE_DOWN': [1, 0],  # Move down
           'STAY': [0, 0]  # don't move
           }
# bgr
DEFAULT_COLOURS = {' ': [0, 0, 0],  # Black background
                   'S': [101, 67, 254],  # stag
                   'H': [178, 196, 47],  # hare1
                   'G': [178, 196, 47],  # hare2
                   'Y': [216, 30, 54],  # young
                   'M': [159, 67, 255],  # mature
                   'C': [238, 133, 114],  # chonghe
                   'D': [238, 133, 114],  # chonghe2
                   'E': [101, 67, 254],  # ecalation

                   # Colours for agents. R value is a unique identifier
                   '1': [166, 90, 3],  
                   '2': [30, 191, 252],  # Blue
                   '3': [204, 168, 0],
                   '4': [154, 157, 252]}  
TRUN_MATURE = 0.3
TRUN_DEATH = 0.3 

def other(player):
    return 0 if player == 1 else 1

def save_img(rgb_arr, path, name):
    plt.imshow(rgb_arr, interpolation='nearest')
    plt.savefig(path + name)

def make_gif_from_image_dir(gif_path, img_folder, gif_name='trajectory', duration=0.2):
    """
    Create a video from a directory of images
    """
    
    print("Rendering gif...")
    if gif_path[-1] != '/':
        gif_path += '/'
    gif_path = gif_path + gif_name + '.gif'
    
    images = [img for img in os.listdir(img_folder) if img.endswith(".png")]
    images.sort()

    rgb_imgs = []
    for i, image in enumerate(images):
        img = cv2.imread(os.path.join(img_folder, image))
        rgb_imgs.append(img)
 
    imageio.mimsave(gif_path, rgb_imgs, 'GIF', duration = duration)


def make_video_from_image_dir(vid_path, img_folder, video_name='trajectory', fps=5):
    """
    Create a video from a directory of images
    """
    images = [img for img in os.listdir(img_folder) if img.endswith(".png")]
    images.sort()

    rgb_imgs = []
    for i, image in enumerate(images):
        img = cv2.imread(os.path.join(img_folder, image))
        rgb_imgs.append(img)

    make_video_from_rgb_imgs(rgb_imgs, vid_path, video_name=video_name, fps=fps)


def make_video_from_rgb_imgs(rgb_arrs, vid_path, video_name='trajectory',
                             fps=5, format="mp4v", resize=(640, 480)):
    """
    Create a video from a list of rgb arrays
    """
    print("Rendering video...")
    if vid_path[-1] != '/':
        vid_path += '/'
    video_path = vid_path + video_name + '.mp4'

    if resize is not None:
        width, height = resize
    else:
        frame = rgb_arrs[0]
        height, width, layers = frame.shape

    fourcc = cv2.VideoWriter_fourcc(*format)
    video = cv2.VideoWriter(video_path, fourcc, float(fps), (width, height))

    for i, image in enumerate(rgb_arrs):
        percent_done = int((i / len(rgb_arrs)) * 100)
        if percent_done % 20 == 0:
            print("\t...", percent_done, "% of frames rendered")
        if resize is not None:
            image = cv2.resize(image, resize, interpolation=cv2.INTER_NEAREST)
        video.write(image)

    video.release()
    cv2.destroyAllWindows()


def return_view(grid, pos, row_size, col_size):
    """Given a map grid, position and view window, returns correct map part

    Note, if the agent asks for a view that exceeds the map bounds,
    it is padded with zeros

    Parameters
    ----------
    grid: 2D array
        map array containing characters representing
    pos: list
        list consisting of row and column at which to search
    row_size: int
        how far the view should look in the row dimension
    col_size: int
        how far the view should look in the col dimension

    Returns
    -------
    view: (np.ndarray) - a slice of the map for the agent to see
    """
    x, y = pos
    left_edge = x - col_size
    right_edge = x + col_size
    top_edge = y - row_size #top=down ? #TODOSSD: why
    bot_edge = y + row_size #bot=up ?
    pad_mat, left_pad, top_pad = pad_if_needed(left_edge, right_edge,
                                               top_edge, bot_edge, grid)
    x += left_pad
    y += top_pad
    view = pad_mat[x - col_size: x + col_size + 1,
                   y - row_size: y + row_size + 1]
    return view


def pad_if_needed(left_edge, right_edge, top_edge, bot_edge, matrix):
    # FIXME(ev) something is broken here, I think x and y are flipped
    row_dim = matrix.shape[0]
    col_dim = matrix.shape[1]
    left_pad, right_pad, top_pad, bot_pad = 0, 0, 0, 0
    if left_edge < 0:
        left_pad = abs(left_edge)
    if right_edge > row_dim - 1:
        right_pad = right_edge - (row_dim - 1) #left-right <-> rol ?
    if top_edge < 0:
        top_pad = abs(top_edge)
    if bot_edge > col_dim - 1:
        bot_pad = bot_edge - (col_dim - 1) #top-bot <-> col ?

    return pad_matrix(left_pad, right_pad, top_pad, bot_pad, matrix, 0), left_pad, top_pad


def pad_matrix(left_pad, right_pad, top_pad, bot_pad, matrix, const_val=1):
    pad_mat = np.pad(matrix, ((left_pad, right_pad), (top_pad, bot_pad)),
                     'constant', constant_values=(const_val, const_val))
    return pad_mat

"""Base class for an agent that defines the possible actions. """


class Agent(object):

    def __init__(self, agent_id, start_pos, grid, env_name):
        """Superclass for all agents.

        Parameters
        ----------
        agent_id: (str)
            a unique id allowing the map to identify the agents
        start_pos: (np.ndarray)
            a 2d array indicating the x-y position of the agents
        grid: (2d array)
            a reference to this agent's view of the environment
        row_size: (int)
            how many rows up and down the agent can look
        col_size: (int)
            how many columns left and right the agent can look
        """
        self.agent_id = agent_id
        self.done = False
        self.pos = np.array(start_pos)
        # TODO(ev) change grid to env, this name is not very informative
        self.grid = grid
        self.reward_this_turn = 0
        self.collective_return = 0
        self.env_name = env_name
        self.update_agent_pos(start_pos)
        self.action_space = Discrete(5)
        self.observation_space = [6]       

    def action_map(self, action_number):
        """Maps action_number to a desired action in the map"""
        return AGENT_ACTIONS[action_number]

    def get_total_actions(self):
        return len(AGENT_ACTIONS)

    def compute_reward(self):
        reward = self.reward_this_turn
        self.collective_return += reward
        self.reward_this_turn = 0
        return reward

    def set_pos(self, new_pos):
        self.pos = np.array(new_pos)

    def get_pos(self):
        return self.pos

    def get_done(self):
        return self.done

    def update_agent_pos(self, new_pos):
        """Updates the agents internal positions

        Returns
        -------
        new_pos: (np.ndarray)
            2 element array describing the agent positions
        old_pos: (np.ndarray)
            2 element array describing where the agent used to be
        """
        ego_new_pos = new_pos  # self.translate_pos_to_egocentric_coord(new_pos)
        new_row, new_col = ego_new_pos
        '''
        if new_row >= self.grid.shape[0]:
            new_row = self.grid.shape[0] - new_row
        elif new_row<0:
            new_row = self.grid.shape[0] + new_row
        if new_col >= self.grid.shape[1]:
            new_col = self.grid.shape[1] - new_col
        elif new_col<0:
            new_col = self.grid.shape[1] + new_col
        temp_pos = [new_row, new_col]
        '''
        # you can't walk through walls
        temp_pos = new_pos.copy()
        if new_row < 0 or new_row >= self.grid.shape[0] or new_col < 0 or new_col >= self.grid.shape[1]:
            temp_pos = self.get_pos()
        
        self.set_pos(temp_pos)


class GridWorldEnv(object):
    def __init__(self, args, choose=0, length = 5):
        self.env_name = args.env_name
        self.num_agents = args.num_agents
        self.episode_length = args.episode_length
        self.length = length      
        self.color_map = DEFAULT_COLOURS          
        self.share_reward = args.share_reward        
        self.shape_reward = args.shape_reward
        self.shape_beta = args.shape_beta

        self.coop = 1
        self.coop_length = 0
        self.defect_coef = -0.9
        self.reward_randomization = args.reward_randomization
        if self.reward_randomization:
            coop = [1,1,0,1,1,1]
            defect_coef = [0, -2, 1, -0.5, 1, 5]
            self.coop = coop[choose]
            self.defect_coef = defect_coef[choose]

        self.max_life = 20
        self.coop_num = 0       
        self.reset_map()
        self.setup_agents()       

    @property
    def action_space(self):
        action_space = []
        for agent in self.agents:
            action_space.append(agent.action_space)
        return action_space

    @property
    def observation_space(self):
        observation_space = []
        for agent in self.agents:
            observation_space.append(agent.observation_space)
        return observation_space

    def Escalation_setup_map(self):
        self.agents_start_pos = []        
        points = []
        num_index = 0
        while num_index < (self.num_agents+1):
            index = np.random.randint(0, self.length, (2)).tolist()
            if (index in points):
                continue
            else:
                points.append(index)
                num_index += 1
        
        for i in range(self.num_agents):
            self.agents_start_pos.append(points[i])

        self.base_map[points[-1][0], points[-1][1]] = 'E'
        self.escalation_points = 1
        self.escalation_pos = np.array(points[-1])

    def setup_agents(self):
        self.coop_num = 0
        self.gore1_num = 0
        self.gore2_num = 0
        self.hare1_num = 0
        self.hare2_num = 0
        self.coop_length = 0
        self.agents = []
        for i in range(self.num_agents):
            agent = Agent(i, self.agents_start_pos[i], self.base_map, self.env_name)
            self.agents.append(agent)

    def map_to_colors(self, base_map=None, color_map=None):
        """Converts a map to an array of RGB values.
        Parameters
        ----------
        map: np.ndarray
            map to convert to colors
        color_map: dict
            mapping between array elements and desired colors
        Returns
        -------
        arr: np.ndarray
            3-dim numpy array consisting of color map
        """
        if base_map is None:
            base_map = self.get_map_with_agents()
        if color_map is None:
            color_map = self.color_map

        rgb_arr = np.zeros((base_map.shape[0], base_map.shape[1], 3), dtype=int)
        for row_elem in range(base_map.shape[0]):
            for col_elem in range(base_map.shape[1]):
                rgb_arr[row_elem, col_elem, :] = color_map[base_map[row_elem, col_elem]]

        return rgb_arr

    def get_obs_agent(self, agent_id):
        """Returns observation for agent_id.
        NOTE: Agents should have access only to their local observations
        during decentralised execution.
        """
        # my pos
        my_pos = self.agents[agent_id].pos.tolist()
        # print('pos:', my_pos)
        # other pos
        other_pos = self.agents[1-agent_id].pos.tolist()
        # escalation pos
        escalation_pos = self.escalation_pos.tolist()            
        #return np.concatenate([my_pos]+[other_pos]+[escalation_pos]+[[self.coop_length]])
        id = my_pos[0]+my_pos[1]*self.length + \
            other_pos[0]*(self.length**2)+other_pos[1]*(self.length**3) + \
            escalation_pos[0]*(self.length**4)+escalation_pos[1]*(self.length**5)
        # print('id:', id)
        return np.array([id, np.concatenate([my_pos]+[other_pos]+[escalation_pos])])
            
    def reset_map(self):
        """Resets the map to be empty as well as a custom reset set by subclasses"""
        self.base_map = np.full((self.length, self.length),' ')
        self.Escalation_setup_map()
        

    def get_map_with_agents(self):
        """Gets a version of the environment map where generic
        'P' characters have been replaced with specific agent IDs.

        Returns:
            2D array of strings representing the map.
        """
        map_with_agents = np.copy(self.base_map)

        for i in range(self.num_agents):
            char_id = str(i + 1) # agent-i
            if map_with_agents[self.agents[i].pos[0], self.agents[i].pos[1]] == ' ':
                map_with_agents[self.agents[i].pos[0], self.agents[i].pos[1]] = char_id
            elif map_with_agents[self.agents[i].pos[0], self.agents[i].pos[1]] == 'E':
                map_with_agents[self.agents[i].pos[0], self.agents[i].pos[1]] = '4'
            else:
                map_with_agents[self.agents[i].pos[0], self.agents[i].pos[1]] = '3'

        return map_with_agents
   
    def update_moves(self, agent_actions):
        for agent_id, action in agent_actions.items():
            agent = self.agents[agent_id]
            selected_action = ACTIONS[action]
            new_pos = agent.get_pos() + selected_action
            # allow the agents to confirm what position they can move to               
            agent.update_agent_pos(new_pos)

    def EscalationUpdateMap(self):
        actions = [[0,1],[0,-1],[-1,0],[1,0]]
        last_pos = self.escalation_pos      
        while self.escalation_points == 0:
            next_choose = np.random.randint(0,4)          
            next_pos = last_pos + actions[next_choose]
            next_row, next_col = next_pos
            if next_row < 0 or next_row >= self.length or next_col < 0 or next_col >= self.length:
                self.escalation_points = 0
            else:
                self.base_map[last_pos[0], last_pos[1]] = ' '
                self.escalation_points = 1
                self.escalation_pos = next_pos
                self.base_map[next_pos[0], next_pos[1]] = 'E'


    def EscalationConsume(self, pos0, pos1):
        charA = self.base_map[pos0[0], pos0[1]]
        charB = self.base_map[pos1[0], pos1[1]]

        if charA == 'E':
            if charB == 'E':
                self.agents[0].reward_this_turn += self.coop
                self.agents[1].reward_this_turn += self.coop
                self.coop_length += 1
                self.coop_num += 1
                self.escalation_points = 0
            else:
                if self.coop_length > 0:
                    self.agents[0].reward_this_turn = self.defect_coef * self.coop_length
                    self.agents[0].done = True
                    self.agents[1].done = True
        else:
            if charB == 'E' and self.coop_length > 0:
                self.agents[1].reward_this_turn = self.defect_coef * self.coop_length
                self.agents[0].done = True
                self.agents[1].done = True
    
        
    def close(self):
        self.agents = []
        return None

    def seed(self, seed):
        if seed is None:
            np.random.seed(1)
        else:
            np.random.seed(seed)

    def render(self, filename=None):
        """ Creates an image of the map to plot or save.

        Args:
            path: If a string is passed, will save the image
                to disk at this location.
        """
        map_with_agents = self.get_map_with_agents()

        rgb_arr = self.map_to_colors(map_with_agents)
        plt.figure()
        plt.imshow(rgb_arr, interpolation='nearest')        
        
        text = "#Coop. Length L = " + str(self.coop_num) + "/" + str(self.episode_length)        
        plt.text(0, 0, text, fontdict={'size': 10, 'color':  'white'})
        plt.title("Escalation") 
            
        if filename is not None:
            plt.savefig(filename)
        
        return rgb_arr.astype(np.uint8)

    def step(self, actions): #action [1,2,4,3,7]
        """A single environment step. Returns reward, terminated, info."""
        actions = [np.argmax(a) for a in actions]
        agent_actions = {}
        for i in range(self.num_agents):
            agent_action = self.agents[i].action_map(actions[i]) 
            agent_actions[i] = agent_action
               
        # move
        self.update_moves(agent_actions)
                                   
        pos0 = self.agents[0].get_pos().tolist()
        pos1 = self.agents[1].get_pos().tolist()
        self.EscalationConsume(pos0, pos1)
        self.EscalationUpdateMap()

        observations = []
        rewards = []
        dones = []
        infos = {'collective_return': [], 'coop&coop_num': [], 'gore1_num': [], 'gore2_num': [],'hare1_num': [], 'hare2_num': []}
        
        for i in range(self.num_agents):
            observations.append(self.get_obs_agent(i))
            reward = self.agents[i].compute_reward() * 0.1
            rewards.append(reward)
            dones.append(self.agents[i].get_done())
            
        collective_return = 0
        for i in range(self.num_agents):
            collective_return += self.agents[i].collective_return

        infos['collective_return'] = collective_return
        infos['coop&coop_num'] = self.coop_num
            
        global_reward = np.sum(rewards)  
        if self.share_reward:
            rewards = [global_reward] * self.num_agents

        if self.shape_reward:
            rewards = list(map(lambda x :x[0] * self.shape_beta + x[1] * (1-self.shape_beta), zip([global_reward] * self.num_agents, rewards)))

        return observations, rewards, dones, infos

    def reset(self):
        """Reset the environment. Required after each full episode.
        Returns initial observations and states.
        """   
        self.reset_map()
        self.setup_agents()

        observations = []
        
        for i in range(self.num_agents):
            observations.append(self.get_obs_agent(i))
        
        return observations


class QLearner(object):

    def __init__(self, env, discount=.9, init_lr=.1, lr_decay=.00001, lr_const=.001):
        self.env = env
        self.discount = discount
        self.init_lr = init_lr
        self.lr_const = lr_const
        self.lr_decay = lr_decay

        self.episodes = 0
        self.iteration = 0
        self.rounds = 0

        state, _ = env.reset()
        self.state = state
        self.rewards = [0, 0]
        self.coop_num_cnt = 0
        self.collective_reward = 0.
        # self.reward_A = 0.
        # self.reward_B = 0.
        STATE_SPACE = env.length**6
        self.Qs = [
            np.random.normal(
                loc=0., scale=3.,
                size=(STATE_SPACE, len(AGENT_ACTIONS), len(AGENT_ACTIONS)))
            # np.ones((env.STATE_SPACE, env.ACTION_SPACE, env.ACTION_SPACE))
            for _ in range(2)
        ]

    def get_lr(self):
        return self.init_lr * exp(-self.lr_decay*self.episodes) + self.lr_const

    def _select_actions(self, state=None):
        return [np.zeros(5), np.zeros(5)]

    def _QtoV(self, state, player):
        return 0

    def _update_Qs(self, old_state, actions, next_state, rewards):
        for i in range(2):
            q = self.get_lr() * (rewards[i] + self.discount * self._QtoV(next_state, i))
            # if old_state[0] == 71:
            # print(old_state[0], actions, q, rewards)
            q += (1-self.get_lr())*self.Qs[i][old_state[0], actions[0], actions[1]]

            self.Qs[i][old_state[0], actions[0], actions[1]] = q

    def train(self):
        self.episodes += 1
        self.iteration += 1

        old_state = self.state
        actions = self._select_actions()
        # print("actions:", actions)
        acts = []
        A = np.zeros(len(AGENT_ACTIONS))
        A[actions[0]] = 1
        B = np.zeros(len(AGENT_ACTIONS))
        B[actions[1]] = 1
        acts = [A, B]
        (next_state, _) , rewards, dones, infos = self.env.step(acts)
        self.rewards = rewards
        # self.reward_A += rewards[0]
        # self.reward_B += rewards[1]
        # print("rewards:", rewards)
        self.state = next_state
        self._update_Qs(old_state, actions, next_state, rewards)

        if self.iteration >= 10000:
            self.env.reset()
            self.rounds += 1
            self.coop_num_cnt = infos['coop&coop_num']
            self.collective_reward = infos['collective_return']
            self.iteration = 0

        if dones[0]:
            self.rounds += 1
            self.env.reset()
            # self.reward_A = 0.
            # self.reward_B = 0.
            self.coop_num_cnt = infos['coop&coop_num']
            self.collective_reward = infos['collective_return']
            self.iteration = 0
            # print(rewards)


class FriendQ(QLearner):

    def __init__(self, env, discount=.95, init_lr=.1, lr_decay=.000005, lr_const=.001):
        super(FriendQ, self).__init__(env, discount, init_lr, lr_decay, lr_const)

    def _select_actions(self, state=None):
        s = state if state is not None else self.state
        actions = []
        # arr = self.Qs[0][s.id]
        # argmax = np.argmax(arr)
        # actions.append(argmax // arr.shape[1])
        # arr = self.Qs[1][s.id]
        # argmax = np.argmax(arr)
        # actions.append(argmax % arr.shape[1])
        arr = self.Qs[0][s[0]]
        p = arr - np.min(arr)
        p = p / np.sum(p)
        p = p.flatten()
        a = np.random.choice(len(p), p=p)
        actions.append(a // arr.shape[1])

        arr = self.Qs[1][s[0]]
        p = arr - np.min(arr)
        p = p / np.sum(p)
        p = p.flatten()
        a = np.random.choice(len(p), p=p)
        actions.append(a % arr.shape[1])

        return actions

    def _QtoV(self, state, player):
        v = np.max(self.Qs[player][state[0]])
        return v

class Args(object):
    def __init__(self):
        self.env_name = 'EscalationGW'
        self.num_agents = 2
        self.episode_length = 10
        self.share_reward = False
        self.shape_reward = False
        self.shape_beta = 0.8
        self.reward_randomization = False

if __name__ == "__main__":
    TIME_LIMIT = 100000000

    args = Args()
    env = GridWorldEnv(args)
    learner = FriendQ(env)

    q_values = []
    lr_values = []
    # rewards_A = []
    # rewards_B = []
    coop_nums = []
    rewards = []
    episode = 0
    for t in range(TIME_LIMIT):
        # if t % 100 == 0:
        #     sys.stdout.write('\r'+str(t))
        #     q = learner.Qs[0][7816, 1, 4]
        #     q_values.append(q)
            # print(learner.rewards)
            # reward_A += learner.rewards[0]
            # rewards_A.append(learner.reward_A)
            # reward_B += learner.rewards[1]
            # rewards_B.append(learner.reward_B)
            # coop_nums.append(learner.coop_num_cnt)
            # collective_rewards.append(learner.collective_reward)
            # lr_values.append(learner.get_lr())
        e = learner.rounds
        if e > episode and e == episode + 1:
            sys.stdout.write('\r'+"Step "+str(t)+", Episode "+str(e))
            q = learner.Qs[0][7816, 1, 4]
            q_values.append(q)
            coop_nums.append(learner.coop_num_cnt)
            rewards.append(learner.collective_reward)
            lr_values.append(learner.get_lr())
            episode = e

        learner.train()

    episodes = [e for e in range(episode)]

    q_values = np.array(q_values)
    errs = np.abs(q_values[1:] - q_values[:-1])

    now_time = datetime.now()
    time_name = str(now_time.month)+"_"+str(now_time.day)+"_"+str(now_time.hour)+"_"+str(now_time.minute)
    data_results_friendQ = pd.concat([episodes, errs, rewards, coop_nums], ignore_index=True)
    data_results_friendQ.to_csv('Friend_QL_{}.csv'.format(time_name))

    print(len(rewards))

    # for row in learner.Qs[0][71].tolist():
    #     print(',\t'.join([str(round(col,2)) for col in row]))
    #     print

    # print('\n')

    # for row in learner.Qs[1][71].tolist():
    #     print(',\t'.join([str(round(col,2)) for col in row]))
    #     print

    # df = pd.DataFrame({'t': range(len(errs)),
    #                 "Err": errs
    #                 })
    # df = df[df['Err']>0]
    # df['t'] = df['t'] * 100
    # df.head()

    # ax = df.plot(x='t', y='Err', figsize=(10, 7), legend=False, ylim=(0., 1.5), title='Friend-Q')
    # ax.set_xlabel("time")
    # ax.set_ylabel("Error")
    # plt.show()